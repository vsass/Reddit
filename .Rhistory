# Reset parallel plan
plan(sequential)
}
gc()
# Function to calculate summaries for submissions
process_submissions_summaries <- function(year) {
cat("Calculating submission summaries for year:", year, "in parallel\n")
# Get all submission files for the year
submissions_files <- list.files(submissions_folder, pattern = paste0(year, ".*\\.json$"), full.names = TRUE)
plan(multisession, workers = 4)
# Calculate summaries in parallel
submissions_summary <- calculate_summary(submissions_files, is_submission = TRUE)
# Save to Parquet for efficiency
output_path <- file.path(output_folder, paste0("submissions_summary_", year, ".parquet"))
arrow::write_parquet(submissions_summary, output_path)
cat("Saved submission summary file:", output_path, "\n")
# Reset parallel plan
plan(sequential)
}
calculate_summary <- function(file_paths, is_submission) {
all_summaries <- rbindlist(lapply(file_paths, function(file_path) {
# Extract year and month from the file name
month <- stringr::str_extract(file_path, "(\\d{2})(?=\\.json$)")
year <- stringr::str_extract(file_path, "(?<=_)(\\d{4})(?=-\\d{2})")
source_type <- ifelse(is_submission, "submissions", "comments")
# Stream in only the "subreddit" field efficiently
con <- file(file_path, "r")
data <- stream_in(con, pagesize = 1000, verbose = TRUE,
handler = function(df) setDT(df)[, .N, by = subreddit])
close(con)
# Aggregate results directly while attaching metadata
data[, `:=`(year = year, month = month, source_type = source_type)]
return(data)
}), fill = TRUE)
return(all_summaries)
}
# summary stats
process_submissions_summaries(year)
calculate_summary <- function(file_paths, is_submission) {
all_summaries <- rbindlist(lapply(file_paths, function(file_path) {
# Extract year and month from the file name
month <- stringr::str_extract(file_path, "(\\d{2})(?=\\.json$)")
year <- stringr::str_extract(file_path, "(?<=_)(\\d{4})(?=-\\d{2})")
source_type <- ifelse(is_submission, "submissions", "comments")
# Stream in only the "subreddit" field efficiently
con <- file(file_path, "r")
data <- stream_in(con, pagesize = 1000, verbose = TRUE,
handler = function(df) {
# Convert to data.table and aggregate
setDT(df)
counts <- df[, .N, by = subreddit]
result <<- rbindlist(list(result, counts), use.names = TRUE, fill = TRUE)
})
close(con)
# Aggregate final results and add metadata
result[, `:=`(year = year, month = month, source_type = source_type)]
result[, .(total_count = sum(total_count)), by = .(subreddit, year, month, source_type)]
}), fill = TRUE)
return(all_summaries)
}
calculate_summary <- function(file_paths, is_submission) {
all_summaries <- rbindlist(lapply(file_paths, function(file_path) {
# Extract year and month from the file name
month <- stringr::str_extract(file_path, "(\\d{2})(?=\\.json$)")
year <- stringr::str_extract(file_path, "(?<=_)(\\d{4})(?=-\\d{2})")
source_type <- ifelse(is_submission, "submissions", "comments")
# Stream in only the "subreddit" field efficiently
con <- file(file_path, "r")
data <- stream_in(con, pagesize = 1000, verbose = TRUE,
handler = function(df) {
# Convert to data.table and aggregate
setDT(df)
counts <- df[, .N, by = subreddit]
result <<- rbindlist(list(result, counts), use.names = TRUE, fill = TRUE)
})
close(con)
# Aggregate final results and add metadata
result[, `:=`(year = year, month = month, source_type = source_type)]
result[, .(total_count = sum(total_count)), by = .(subreddit, year, month, source_type)]
}), fill = TRUE)
return(all_summaries)
}
# Function to calculate summaries for submissions
process_submissions_summaries <- function(year) {
cat("Calculating submission summaries for year:", year, "in parallel\n")
# Get all submission files for the year
submissions_files <- list.files(submissions_folder, pattern = paste0(year, ".*\\.json$"), full.names = TRUE)
plan(multisession, workers = 4)
# Calculate summaries in parallel
submissions_summary <- calculate_summary(submissions_files, is_submission = TRUE)
# Save to Parquet for efficiency
output_path <- file.path(output_folder, paste0("submissions_summary_", year, ".parquet"))
arrow::write_parquet(submissions_summary, output_path)
cat("Saved submission summary file:", output_path, "\n")
# Reset parallel plan
plan(sequential)
}
# summary stats
process_submissions_summaries(year)
calculate_summary <- function(file_paths, is_submission) {
all_summaries <- rbindlist(lapply(file_paths, function(file_path) {
# Extract metadata from the filename
month <- stringr::str_extract(file_path, "(\\d{2})(?=\\.json$)")
year <- stringr::str_extract(file_path, "(?<=_)(\\d{4})(?=-\\d{2})")
source_type <- ifelse(is_submission, "submissions", "comments")
# Initialize result container
result <- data.table(subreddit = character(), total_count = integer())
# Stream in JSON and filter only relevant fields
con <- file(file_path, "r")
stream_in(con, pagesize = 1000, verbose = FALSE,
handler = function(df) {
# Convert to data.table and keep only relevant columns
setDT(df)
if (all(c("subreddit", "id") %in% names(df))) {
counts <- df[, .N, by = subreddit]
result <<- rbindlist(list(result, counts), use.names = TRUE, fill = TRUE)
}
},
simplifyVector = TRUE)
close(con)
# Aggregate and attach metadata
if (nrow(result) > 0) {
result[, `:=`(year = year, month = month, source_type = source_type)]
result <- result[, .(total_count = sum(total_count)),
by = .(subreddit, year, month, source_type)]
}
return(result)
}), fill = TRUE)
return(all_summaries)
}
gc()
# summary stats
process_submissions_summaries(year)
close(con)
gc()
calculate_summary <- function(file_paths, is_submission) {
all_summaries <- rbindlist(lapply(file_paths, function(file_path) {
# Extract metadata from the filename
month <- stringr::str_extract(file_path, "(\\d{2})(?=\\.json$)")
year <- stringr::str_extract(file_path, "(?<=_)(\\d{4})(?=-\\d{2})")
source_type <- ifelse(is_submission, "submissions", "comments")
# Initialize result container
result <- data.table(subreddit = character(), total_count = integer())
# Stream in JSON and filter only relevant fields
con <- file(file_path, "r")
stream_in(con, pagesize = 1000, verbose = TRUE,
handler = function(df) {
# Convert to data.table and keep only relevant columns
setDT(df)
if (all(c("subreddit", "id") %in% names(df))) {
counts <- df[, .N, by = subreddit]
result <<- rbindlist(list(result, counts), use.names = TRUE, fill = TRUE)
}
},
simplifyVector = TRUE)
close(con)
# Aggregate and attach metadata
if (nrow(result) > 0) {
result[, `:=`(year = year, month = month, source_type = source_type)]
result <- result[, .(total_count = sum(total_count)),
by = .(subreddit, year, month, source_type)]
}
return(result)
}), fill = TRUE)
return(all_summaries)
}
# summary stats
process_submissions_summaries(year)
warnings()
gc()
# Helper function to read and filter both types (submissions & comments) of JSON files
read_and_filter <- function(file_path, keywords, columns_to_keep, is_submission) {
# Read the JSON file as a data.table
con <- file(file_path, "r")
data <- stream_in(con, verbose = TRUE)
close(con)
# Select relevant columns before converting to data.table
selected_cols <- intersect(names(data), columns_to_keep)
data <- data[, selected_cols, drop = FALSE]  # subset before conversion
# Convert to data.table only for relevant columns
setDT(data)
# Summarize the count of submissions by subreddit before filtering
subreddit_summary <- data.table(subreddit = data$subreddit) |>
.[, .N, by = subreddit]  # N counts rows per subreddit
# Save the subreddit summary as a Parquet file
summary_output_path <- file.path(output_folder, paste0("subreddit_summary_", basename(file_path), ".parquet"))
arrow::write_parquet(subreddit_summary, summary_output_path, compression = "zstd")
cat("Saved subreddit summary file:", summary_output_path, "\n")
# Filter submissions or comments
if (is_submission) {
# Combine title and selftext, filter by keyword
data[, text := paste(title, selftext, sep = ": ")]
data[, c("title", "selftext") := NULL]
data <- data[str_detect(text, keywords)]
} else {
# Load the master list of submission IDs as a data.table
all_submission_ids <- fread(file.path(output_folder, "all_submission_ids.parquet"))$submission_id
data[, submission_id := str_remove(link_id, "^t3_")]
data <- data[submission_id %in% all_submission_ids]
}
# Convert all columns to characters to prevent incompatible types
data <- data[, lapply(.SD, as.character)]
return(data)
}
# Function to process submissions data for a year, saving monthly files
process_year_monthly_submissions <- function(year, keywords, submission_columns) {
cat("Processing submissions for year:", year, "\n\n")
# Get all monthly submission files
submission_files <- list.files(submissions_folder, pattern = paste0(year, ".*\\.json$"), full.names = TRUE)
# Set up parallel plan
plan(multisession, workers = 4)
# Process and save monthly submissions
future_walk(submission_files, function(file) {
monthly_submissions <- read_and_filter(
file_path = file,
keywords = keywords,
columns_to_keep = submission_columns,
is_submission = TRUE
)
# Extract month
month <- str_extract(file, pattern = "(\\d{2})(?=\\.json$)")
output_path <- file.path(output_folder, paste0("submissions_", "2011", "_", "06", ".parquet"))
# Save as Parquet
arrow::write_parquet(monthly_submissions, output_path, compression = "zstd")
cat("Saved monthly submissions file:", output_path, "\n")
})
# Reset parallel plan
plan(sequential)
}
# enter manually to keep track of storage space
year <- "2012"
# process monthly files and combine by year
process_year_monthly_submissions(2012, keywords = keywords, submission_columns = submission_columns)
# Get all monthly submission files
submission_files <- list.files(submissions_folder, pattern = paste0(year, ".*\\.json$"), full.names = TRUE)
submission_files
basename(file_path)
basename(submission_files)
# Set up parallel plan
plan(multisession, workers = 4)
# Read the JSON file as a data.table
con <- file("data/raw/submissions/RS_2012-01.json", "r")
data <- stream_in(con, verbose = TRUE)
year
# Function to process submissions data for a year, saving monthly files
process_year_monthly_submissions <- function(year, keywords, submission_columns) {
cat("Processing submissions for year:", year, "\n\n")
# Get all monthly submission files
submission_files <- list.files(submissions_folder, pattern = paste0(year, ".*\\.json$"), full.names = TRUE)
# Set up parallel plan
plan(multisession, workers = 4)
# Process and save monthly submissions
future_walk(submission_files, function(file) {
monthly_submissions <- read_and_filter(
file_path = file,
keywords = keywords,
columns_to_keep = submission_columns,
is_submission = TRUE
)
# Extract month
month <- str_extract(file, pattern = "(\\d{2})(?=\\.json$)")
output_path <- file.path(output_folder, paste0("submissions_", year, "_", month, ".parquet"))
# Save as Parquet
arrow::write_parquet(monthly_submissions, output_path, compression = "zstd")
cat("Saved monthly submissions file:", output_path, "\n")
})
# Reset parallel plan
plan(sequential)
}
# Select relevant columns before converting to data.table
selected_cols <- intersect(names(data), columns_to_keep)
columns_to_keep = submission_columns
columns_to_keep
# Select relevant columns before converting to data.table
selected_cols <- intersect(names(data), columns_to_keep)
selected_cols
data <- data[, selected_cols, drop = FALSE]  # subset before conversion
data
# Convert to data.table only for relevant columns
setDT(data)
# Summarize the count of submissions by subreddit before filtering
subreddit_summary <- data.table(subreddit = data$subreddit) |>
.[, .N, by = subreddit]  # N counts rows per subreddit
# Summarize the count of submissions by subreddit before filtering
subreddit_summary <- data.table(subreddit = data$subreddit) |> .[, .N, by = subreddit]  # N counts rows per subreddit
data.table(subreddit = data$subreddit)
# Summarize the count of submissions by subreddit before filtering
subreddit_summary <- data.table(subreddit = data$subreddit)
subreddit_summary[, .N, by = subreddit]  # N counts rows per subreddit
is_submission == TRUE
is_submission = TRUE
file_type <- if_else(is_submission, "submissions_", "comments_")
file_type
summary_output_path <- file.path(output_folder, paste0(file_type, "summary", year, "_", month, ".parquet"))
output_folder
summary_output_path <- file.path(output_folder, paste0(file_type, "summary", year, "_", month, ".parquet"))
year
month
month <- str_extract(file, pattern = "(\\d{2})(?=\\.json$)")
month <- str_extract("data/raw/submissions/RS_2012-01.json", pattern = "(\\d{2})(?=\\.json$)")
month
summary_output_path <- file.path(output_folder, paste0(file_type, "summary", year, "_", month, ".parquet"))
summary_output_path
gc()
# Reset parallel plan
plan(sequential)
gc()
View(subreddit_summary)
# Summarize the count of submissions by subreddit before filtering
subreddit_summary <- data.table(subreddit = data$subreddit)
subreddit_summary <- subreddit_summary[, .N, by = subreddit]  # N counts rows per subreddit
subreddit_summary
View(subreddit_summary)
# Function to process submissions data for a year, saving monthly files
process_year_monthly_submissions <- function(year, keywords, submission_columns) {
cat("Processing submissions for year:", year, "\n\n")
# Get all monthly submission files
submission_files <- list.files(submissions_folder, pattern = paste0(year, ".*\\.json$"), full.names = TRUE)
# Set up parallel plan
plan(multisession, workers = 4)
# Process and save monthly submissions
future_walk(submission_files, function(file) {
monthly_submissions <- read_and_filter(
file_path = file,
keywords = keywords,
columns_to_keep = submission_columns,
is_submission = TRUE
)
# Extract month
month <- str_extract(file, pattern = "(\\d{2})(?=\\.json$)")
output_path <- file.path(output_folder, paste0("submissions_", year, "_", month, ".parquet"))
# Save as Parquet
arrow::write_parquet(monthly_submissions, output_path, compression = "zstd")
cat("Saved monthly submissions file:", output_path, "\n")
})
# Reset parallel plan
plan(sequential)
}
# Helper function to read and filter both types (submissions & comments) of JSON files
read_and_filter <- function(file_path, keywords, columns_to_keep, is_submission) {
# Read the JSON file as a data.table
con <- file(file_path, "r")
data <- stream_in(con, verbose = TRUE)
close(con)
# Select relevant columns before converting to data.table
selected_cols <- intersect(names(data), columns_to_keep)
data <- data[, selected_cols, drop = FALSE]  # subset before conversion
# Convert to data.table only for relevant columns
setDT(data)
# Summarize the count of submissions by subreddit before filtering
subreddit_summary <- data.table(subreddit = data$subreddit)
subreddit_summary <- subreddit_summary[, .N, by = subreddit]  # N counts rows per subreddit
month <- str_extract(file_path, pattern = "(\\d{2})(?=\\.json$)")
file_type <- if_else(is_submission, "submissions_", "comments_")
summary_output_path <- file.path(output_folder, paste0(file_type, "summary_", year, "_", month, ".parquet"))
# Save the subreddit summary as a Parquet file
arrow::write_parquet(subreddit_summary, summary_output_path, compression = "zstd")
cat("Saved subreddit summary file:", summary_output_path, "\n")
# Filter submissions or comments
if (is_submission) {
# Combine title and selftext, filter by keyword
data[, text := paste(title, selftext, sep = ": ")]
data[, c("title", "selftext") := NULL]
data <- data[str_detect(text, keywords)]
} else {
# Load the master list of submission IDs as a data.table
all_submission_ids <- fread(file.path(output_folder, "all_submission_ids.parquet"))$submission_id
data[, submission_id := str_remove(link_id, "^t3_")]
data <- data[submission_id %in% all_submission_ids]
}
# Convert all columns to characters to prevent incompatible types
data <- data[, lapply(.SD, as.character)]
return(data)
}
year
rm(month, files_type, columns_to_keep, is_submission, selected_cols, submission_files, summary_output_path)
rm(month, file_type, columns_to_keep, is_submission, selected_cols, submission_files, summary_output_path)
gc()
gc()
# Helper function to read and filter both types (submissions & comments) of JSON files
read_and_filter <- function(file_path, keywords, columns_to_keep, is_submission) {
# Read the JSON file as a data.table
con <- file(file_path, "r")
data <- stream_in(con, verbose = TRUE)
close(con)
# Select relevant columns before converting to data.table
selected_cols <- intersect(names(data), columns_to_keep)
data <- data[, selected_cols, drop = FALSE]  # subset before conversion
# Convert to data.table only for relevant columns
setDT(data)
# Summarize the count of submissions by subreddit before filtering
subreddit_summary <- data.table(subreddit = data$subreddit)
subreddit_summary <- subreddit_summary[, .N, by = subreddit]  # N counts rows per subreddit
month <- str_extract(file_path, pattern = "(\\d{2})(?=\\.json$)")
file_type <- if_else(is_submission, "submissions_", "comments_")
summary_output_path <- file.path(output_folder, paste0(file_type, "summary_", year, "_", month, ".parquet"))
# Save the subreddit summary as a Parquet file
arrow::write_parquet(subreddit_summary, summary_output_path, compression = "zstd")
cat("Saved subreddit summary file:", summary_output_path, "\n")
# Filter submissions or comments
if (is_submission) {
# Combine title and selftext, filter by keyword
data[, text := paste(title, selftext, sep = ": ")]
data[, c("title", "selftext") := NULL]
data <- data[str_detect(text, keywords)]
} else {
# Load the master list of submission IDs as a data.table
all_submission_ids <- fread(file.path(output_folder, "all_submission_ids.parquet"))$submission_id
data[, submission_id := str_remove(link_id, "^t3_")]
data <- data[submission_id %in% all_submission_ids]
}
# Convert all columns to characters to prevent incompatible types
data <- data[, lapply(.SD, as.character)]
return(data)
}
# Function to process submissions data for a year, saving monthly files
process_year_monthly_submissions <- function(year, keywords, submission_columns) {
cat("Processing submissions for year:", year, "\n\n")
# Get all monthly submission files
submission_files <- list.files(submissions_folder, pattern = paste0(year, ".*\\.json$"), full.names = TRUE)
# Set up parallel plan
plan(multisession, workers = 4)
# Process and save monthly submissions
future_walk(submission_files, function(file) {
monthly_submissions <- read_and_filter(
file_path = file,
keywords = keywords,
columns_to_keep = submission_columns,
is_submission = TRUE
)
# Extract month
month <- str_extract(file, pattern = "(\\d{2})(?=\\.json$)")
output_path <- file.path(output_folder, paste0("submissions_", year, "_", month, ".parquet"))
# Save as Parquet
arrow::write_parquet(monthly_submissions, output_path, compression = "zstd")
cat("Saved monthly submissions file:", output_path, "\n")
})
# Reset parallel plan
plan(sequential)
}
# process monthly files and combine by year
process_year_monthly_submissions(2012, keywords = keywords, submission_columns = submission_columns)
# check that combination worked
open_dataset("data/filtered/submissions_2012_01.parquet") |> glimpse() #  submissions
# check that combination worked
open_dataset("data/filtered/submissions_2012_01.parquet") |> slice(6) |> pull(text) #  submissions
# check that combination worked
open_dataset("data/filtered/submissions_2012_01.parquet") |> .[6, text] #  submissions
# check that combination worked
open_dataset("data/filtered/submissions_2012_01.parquet") |> [6, text] #  submissions
# check that combination worked
open_dataset("data/filtered/submissions_2012_01.parquet")[6, text] #  submissions
# check that combination worked
open_dataset("data/filtered/submissions_2012_01.parquet")[6, "text"] #  submissions
# check that combination worked
open_dataset("data/filtered/submissions_2012_11.parquet") |> glimpse() #  submissions
# check that combination worked
open_dataset("data/filtered/submissions_summary_2012_05.parquet") |> glimpse() #  submissions
# Combine monthly files into yearly Parquet files
combine_yearly_files <- function(year, file_type) {
cat("Combining monthly files for year:", year, "\n")
# Read and filter all monthly submissions
plan(multisession, workers = 2) # set the parallelization plan
# Combine monthly submissions
files <- list.files(output_folder, pattern = paste0(file_type, "_", year, "_.*\\.parquet$"), full.names = TRUE)
combined_files <- future_map_dfr(files, arrow::read_parquet)
arrow::write_parquet(combined_files, file.path(output_folder, paste0(file_type, "_", year, ".parquet")), compression = "zstd")
cat("Combined and saved yearly", file_type ,"file for", year, "\n")
# Handle subreddit summary files
summary_files <- list.files(output_folder, pattern = paste0("submissions_summary_.*_", year, ".*\\.parquet$"), full.names = TRUE)
# Read and combine all summary files
combined_summary <- rbindlist(lapply(summary_files, arrow::read_parquet))
combined_summary <- combined_summary[, .(N = sum(N)), by = subreddit] # Aggregate by subreddit to get total counts
# Write combined summary to Parquet
summary_output_path <- file.path(output_folder, paste0("submissions_summary_", year, ".parquet"))
arrow::write_parquet(combined_summary, summary_output_path, compression = "zstd")
cat("Combined and saved yearly subreddit summary file for", year, "\n")
# Reset parallel plan to avoid lingering workers
plan(sequential)
}
combine_yearly_files(year = 2012, file_type = "submissions")
# Read and filter all monthly submissions
plan(multisession, workers = 4) # set the parallelization plan
gc()
paste0("submissions_summary_.*_", year, ".*\\.parquet$")
paste0("submissions_summary_", year, ".*\\.parquet$")
# Handle subreddit summary files
summary_files <- list.files(output_folder, pattern = paste0("submissions_summary_", year, ".*\\.parquet$"), full.names = TRUE)
summary_files
# Read and combine all summary files
combined_summary <- future_map_dfr(summary_files, arrow::read_parquet)
View(combined_summary)
combined_summary[, .(N = sum(N)), by = subreddit]
combined_summary <- combined_summary[, .(N = sum(N)), by = subreddit] # Aggregate by subreddit to get total counts
View(combined_summary)
paste0("submissions_summary_", year, ".parquet")
# Write combined summary to Parquet
summary_output_path <- file.path(output_folder, paste0("submissions_summary_", year, ".parquet"))
arrow::write_parquet(combined_summary, summary_output_path, compression = "zstd")
# check that combination worked
open_dataset("data/filtered/submissions_summary_2012.parquet") |> glimpse() #  submissions
rm(subreddit_summary, combined_summary, data)
gc()
# Combine monthly submissions
files <- list.files(output_folder, pattern = paste0(file_type, "_", year, "_.*\\.parquet$"), full.names = TRUE)
combined_files <- future_map_dfr(files, arrow::read_parquet)
# Reset parallel plan to avoid lingering workers
plan(sequential)
gc()
# check that combination worked
open_dataset("data/filtered/submissions_2012.parquet") |> glimpse() #  submissions
close(con)
